{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation \n",
    "\n",
    "> All models are wrong, but some are useful. â€” George E. P. Box\n",
    "\n",
    "During the winter of 2018, I decided to work through the [SKLearn library](http://scikit-learn.org/) in Python and try to rigorously understand most, if not all, of the algorithms implemented in it. To my surprise, I found that it was rather difficult to find comprehensive explorations of the algorithms which were also accessible to non-experts; most blogs skimmed over the mathematical underpinnings of machine learning, and most papers presupposed great familiarity with the field. \n",
    "\n",
    "This section of my blog is devoted to exploring ML in a way that is comprehensive and rigorous, but still practical and accessible to a relatively broad audience. Obviously, not every reader will be interested in every aspect of each post. You might want to simply gain a practical understanding of when to use a certain clustering algorithm; or you might want to learn why expectation-maximization optimization really works. However, I'm hoping that most people will find *something* interesting in these posts. \n",
    "\n",
    "I think there are at least two reasons that it's worth deeply understanding ML algorithms:\n",
    "\n",
    "1. First, it's fun! The math behind statistical inference and machine learning is *really* cool. \n",
    "2. More practically, it will allow you to write more effective code. It's much easier to figure out why your model isn't working if you actually understand how your model works. \n",
    "\n",
    "# Post Structure\n",
    "\n",
    " In order to let readers easily jump around, I will loosely divide each post into the following four sections:\n",
    "\n",
    "1. Motivation and Intuition\n",
    "2. Mathematical Derivation\n",
    "3. Implementation from Scratch (i.e without using machine learning Libraries - this is to make sure we really understand how each algorithms works)\n",
    "4. Practical Use (i.e. using libraries like SKLearn, Tensorflow, Pytorch)\n",
    "\n",
    "It's also worth noting that in both the 1st and 4th sections, I will focus extensively on use cases of each of the algorithms - i.e. the kinds of situations in which you'd use a Naives Bayes Classifier over a SVN or vice versa. Admittedly, not every post will follow this exact structure, but each post should generally cover these kinds of information. \n",
    "\n",
    "# Blog Structure\n",
    "\n",
    "\n",
    "\n",
    "# Background\n",
    "\n",
    "If you're just starting to learn about Data Science, here are a couple pieces of background that you might find handy. \n",
    "\n",
    "## Math \n",
    "\n",
    "## Modeling \n",
    "\n",
    "### Generative Models\n",
    "\n",
    "A **model** is a set of assumptions (and, usually, equations) which frame the way the world works. For example, you might model a series of dice rolls in a board game by assuming they are independent from each other - in other words, the outcome of the first rolls does not affect the outcome of the second roll.\n",
    "\n",
    "However, we'll usually be interested in **generative models.** A generative model with respect to some observed data is a model which is capable of *fully simulating* the observed data. For example, if you observed a bunch of dice rolls in a row, as follows:\n",
    "\n",
    "$$ 1, \\, 3, \\, 4, \\, 6, \\, 2, \\, 5, \\, 5, \\, 3, \\, 1, \\, 6$$\n",
    "\n",
    "just knowing that each roll is independent of the others is *not* enough to simulate the data. To do that, you'd need to also to specify the *probability distribution* of the value of each roll - in other words, the probability that any roll will land as a $1$ or a $2$ or a $3$, etc. \n",
    "\n",
    "We like generative models for at least two reasons! First, they let us do fun (and useful) math - for example, we can estimate the probability that the observed data occurs under a model. As we'll see, calculating the likelihood of observed data is extremely important in techniques like Maximum Likelihood Estimation and more. Second, generative models can also *do* cool things: for example, a generative model for natural language can [write a chapter of Harry Potter](https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6) or even create [fake pictures of real celebrities](https://www.youtube.com/watch?v=VrgYtFhVGmg). \n",
    "\n",
    "### The Data Generating Process\n",
    "\n",
    "The **Data Generating Process** (DGP) is the \"true\" generative model. To be more specific, in most problems we assume there is some underlying joint probability distribution which governs the data we observe, and the DGP is that underlying distribution. The DGP is a bit like the government in this sense - we can never know exactly what it's doing internally, but we can use external data to get a rough sense of what's going on.\n",
    "\n",
    "In the example in the next section, which will tie together all of this material, we have a \"God's eye view\" and can see the DGP in all its glory, but only because I literally made up the data. In reality, you will never know the DGP - the point of ML is broadly to create models which approximate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric and Nonparametric Models\n",
    "\n",
    "Like people, models tend to come in families. For example, the normal distribution is not a single distribution - it's a family of extremely similar distributions, each of which depends on two values: a *mean* and *variance.* We generally call values which help index families of models *parameters*. \n",
    "\n",
    "Unfortunately, we don't usually know the values of parameters we're interested in. As a result, we have to *estimate* the parameter \n",
    "\n",
    "[Warning: the term \"estimator\" is extremely confusing. The key point to remember is that an *estimate* is nonrandom, whereas an *estimator* is a function of random data which serves as a guess for a parameter of interest.]{: .btn .btn--warning}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(210)\n",
    "\n",
    "# Simulate data w random noise\n",
    "true_lambda = 457 + np.pi \n",
    "data = np.random.poisson(true_lambda, size = 1000) \n",
    "data = data + 50 * np.random.randn(size = 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "# Last Notes\n",
    "\n",
    "I'm always learning, and I'm sure I will make mistakes in the blog. If you find inaccuracies in my posts, please let me know either by [opening an issue on GitHub](https://github.com/amspector100/amspector100.github.io) (preferred) or emailing me at amspector100@gmail.com. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
