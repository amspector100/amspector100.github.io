
## Intuition and Introduction

*Maximum Likelihood Estimation*, or MLE, is a technique for guessing *unknown parameters* for *models* of observed data. Specifically, as its name suggests, in MLE, you select the value for the unknown parameter which maximizes the probability of the observed data according to the model. To understand how this works intuitively, consider the following example. (You could also just skip straight to the [math](#math)). 

Imagine you flip a penny 100 times, and it lands heads every time. You probably have some internal model of how coin flips work - for example, it's reasonable to assume that each toss is independent of the other coin tosses. However, there's a key parameter in this model you're missing: for a weighted coin, you don't know the probability $p$ that any individual toss will come up as heads. However, you've observed that the penny landed heads 100 times in a row, so you can safely infer that $p$ is pretty close to $1$. This is a simple example of Maximum Likelihood Estimation.




```python
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(0, 100, 1).astype(float)/100
plt.plot(x, x**100, c = 'cornflowerblue')
plt.title('Likelihood of 100 Coins Flips Landing Heads')
plt.xlabel('Likelihood of 1 Flip Landing Heads')
plt.show()

```


![png](2018-09-16-MLE_files/2018-09-16-MLE_1_0.png)


## Math

